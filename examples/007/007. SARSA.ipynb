{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b3c462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:00.086973Z",
     "start_time": "2025-11-12T18:41:59.715261Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from moviepy import ImageSequenceClip # to generate gif\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import matplotlib \n",
    "#matplotlib.use('Qt5Agg') # Activte it if you want external plot for any interaction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa34c6",
   "metadata": {},
   "source": [
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è CartPole-v1 Discretization Function\n",
    "\n",
    "CartPole has **4 continuous state variables**:\n",
    "\n",
    " **1**. Cart position\n",
    "\n",
    " **2**. Cart velocity\n",
    "\n",
    " **3**. Pole angle\n",
    "\n",
    " **4**. Pole angular velocity\n",
    "\n",
    "We‚Äôll discretize these into bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868a3790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:00.091450Z",
     "start_time": "2025-11-12T18:42:00.088029Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discretize_cartpole_state(observation, state_bins):\n",
    "    \"\"\"\n",
    "    Convert a continuous CartPole state into a discrete index for Q-learning.\n",
    "\n",
    "    Parameters:\n",
    "        observation : array-like of shape (4,)\n",
    "            [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
    "        state_bins : list or array-like of length 4\n",
    "            Number of bins for each state dimension, e.g. [6, 6, 12, 6]\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "            A single integer index representing the discretized state.\n",
    "    \"\"\"\n",
    "    cart_pos, cart_vel, pole_ang, pole_ang_vel = observation\n",
    "\n",
    "    # Define reasonable thresholds for state variables\n",
    "    cart_pos_limit = 4.8       # track ends at about ¬±4.8\n",
    "    cart_vel_limit = 3.0       # max reasonable velocity\n",
    "    pole_ang_limit = math.radians(15)  # about ¬±15 degrees\n",
    "    pole_ang_vel_limit = math.radians(50)\n",
    "\n",
    "    # Create bin edges for each state variable\n",
    "    cart_pos_bins = np.linspace(-cart_pos_limit, cart_pos_limit, state_bins[0] + 1)\n",
    "    cart_vel_bins = np.linspace(-cart_vel_limit, cart_vel_limit, state_bins[1] + 1)\n",
    "    pole_ang_bins = np.linspace(-pole_ang_limit, pole_ang_limit, state_bins[2] + 1)\n",
    "    pole_ang_vel_bins = np.linspace(-pole_ang_vel_limit, pole_ang_vel_limit, state_bins[3] + 1)\n",
    "\n",
    "    # Digitize each continuous value into a discrete bucket\n",
    "    cart_pos_idx = np.digitize(cart_pos, cart_pos_bins) - 1\n",
    "    cart_vel_idx = np.digitize(cart_vel, cart_vel_bins) - 1\n",
    "    pole_ang_idx = np.digitize(pole_ang, pole_ang_bins) - 1\n",
    "    pole_ang_vel_idx = np.digitize(pole_ang_vel, pole_ang_vel_bins) - 1\n",
    "\n",
    "    # Clip indices to avoid overflow (in case value exceeds limits)\n",
    "    cart_pos_idx = np.clip(cart_pos_idx, 0, state_bins[0] - 1)\n",
    "    cart_vel_idx = np.clip(cart_vel_idx, 0, state_bins[1] - 1)\n",
    "    pole_ang_idx = np.clip(pole_ang_idx, 0, state_bins[2] - 1)\n",
    "    pole_ang_vel_idx = np.clip(pole_ang_vel_idx, 0, state_bins[3] - 1)\n",
    "\n",
    "    # Combine all indices into a single integer (multi-dimensional ‚Üí 1D)\n",
    "    index = (cart_pos_idx * state_bins[1] * state_bins[2] * state_bins[3] +\n",
    "             cart_vel_idx * state_bins[2] * state_bins[3] +\n",
    "             pole_ang_idx * state_bins[3] +\n",
    "             pole_ang_vel_idx)\n",
    "\n",
    "    return int(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce5c34",
   "metadata": {},
   "source": [
    "### üöó MountainCar-v0 Discretization Function\n",
    "\n",
    "MountainCar has only **2 continuous state variables**:\n",
    "\n",
    " **1**. Car position\n",
    "\n",
    " **2**. Car velocity\n",
    "\n",
    "The agent must learn to move between the valleys using momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a56e5d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:00.104113Z",
     "start_time": "2025-11-12T18:42:00.092298Z"
    }
   },
   "outputs": [],
   "source": [
    "def discretize_mountaincar_state(observation, state_bins):\n",
    "    \"\"\"\n",
    "    Convert a continuous MountainCar state into a discrete index for Q-learning.\n",
    "\n",
    "    Parameters:\n",
    "        observation : array-like of shape (2,)\n",
    "            [position, velocity]\n",
    "        state_bins : list or array-like of length 2\n",
    "            Number of bins for each dimension, e.g. [20, 20]\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "            A single integer index representing the discretized state.\n",
    "    \"\"\"\n",
    "    car_position, car_velocity = observation\n",
    "\n",
    "    # Define the known limits for MountainCar-v0\n",
    "    position_min, position_max = -1.2, 0.6\n",
    "    velocity_min, velocity_max = -0.07, 0.07\n",
    "\n",
    "    # Create bin edges\n",
    "    position_bins = np.linspace(position_min, position_max, state_bins[0] + 1)\n",
    "    velocity_bins = np.linspace(velocity_min, velocity_max, state_bins[1] + 1)\n",
    "\n",
    "    # Digitize (find which bin each variable falls into)\n",
    "    pos_idx = np.digitize(car_position, position_bins) - 1\n",
    "    vel_idx = np.digitize(car_velocity, velocity_bins) - 1\n",
    "\n",
    "    # Clip indices to stay within range\n",
    "    pos_idx = np.clip(pos_idx, 0, state_bins[0] - 1)\n",
    "    vel_idx = np.clip(vel_idx, 0, state_bins[1] - 1)\n",
    "\n",
    "    # Combine into one index\n",
    "    index = pos_idx * state_bins[1] + vel_idx\n",
    "\n",
    "    return int(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4eecc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:00.116162Z",
     "start_time": "2025-11-12T18:42:00.104798Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_q_table(state_bins, num_actions):\n",
    "    \"\"\"\n",
    "    Create and initialize a Q-table for a discretized continuous environment.\n",
    "\n",
    "    Parameters:\n",
    "        state_bins : list or array-like\n",
    "            Number of bins per state dimension, e.g. [6, 6, 12, 6]\n",
    "        num_actions : int\n",
    "            Number of possible discrete actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray\n",
    "            A Q-table of shape (num_states, num_actions), initialized to zeros.\n",
    "    \"\"\"\n",
    "    # Total number of discrete states = product of all bins\n",
    "    num_states = int(np.prod(state_bins))\n",
    "    \n",
    "    # Initialize Q-table with zeros (or small random numbers)\n",
    "    q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "    return q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4142fd7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:00.125970Z",
     "start_time": "2025-11-12T18:42:00.116799Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def epsilon_greedy_action(env, q_table, state_index, epsilon):\n",
    "    \"\"\"\n",
    "    Select an action using the epsilon-greedy policy.\n",
    "\n",
    "    This strategy balances exploration and exploitation:\n",
    "    - With probability Œµ (epsilon), choose a random action (exploration).\n",
    "    - With probability (1 - Œµ), choose the best-known action (exploitation).\n",
    "\n",
    "    Parameters:\n",
    "        env : gym.Env\n",
    "            The environment, used to sample random actions.\n",
    "        q_table : np.ndarray\n",
    "            The Q-table storing estimated action values for each state.\n",
    "            Shape: (num_states, num_actions)\n",
    "        state_index : int\n",
    "            The index of the current discrete state in the Q-table.\n",
    "        epsilon : float\n",
    "            The exploration rate (0 ‚â§ Œµ ‚â§ 1). Higher means more exploration.\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "            The selected action (integer from 0 to env.action_space.n - 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a random number between 0 and 1\n",
    "    random_value = np.random.random()\n",
    "\n",
    "    # With probability epsilon ‚Üí explore (pick a random action)\n",
    "    if random_value < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        # Otherwise ‚Üí exploit (choose the action with the highest Q-value)\n",
    "        action = np.argmax(q_table[state_index])\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d671d989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:00.139726Z",
     "start_time": "2025-11-12T18:42:00.126679Z"
    }
   },
   "outputs": [],
   "source": [
    "def Train_SARSA(env_name, episodes_num, printout, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    if('CartPole' in env_name):\n",
    "        state_bins = [6, 6, 12, 6]\n",
    "    if('MountainCar' in env_name):\n",
    "        state_bins = [20, 20]\n",
    "    # Initializing the Q table\n",
    "    n_actions = env.action_space.n\n",
    "    Q_table = initialize_q_table(state_bins,n_actions)\n",
    "    \n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    \n",
    "    for episode in range(episodes_num):\n",
    "        \n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        if('CartPole' in env_name):\n",
    "            state_idx = discretize_cartpole_state(state, state_bins)\n",
    "        if('MountainCar' in env_name):\n",
    "            state_idx = discretize_mountaincar_state(state, state_bins)\n",
    "            \n",
    "        # epsilon greedy\n",
    "        action = epsilon_greedy_action(env, Q_table, state_idx, epsilon)\n",
    "           \n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated #or truncated\n",
    "            \n",
    "            if('CartPole' in env_name):\n",
    "                next_state_idx = discretize_cartpole_state(next_state, state_bins)\n",
    "            if('MountainCar' in env_name):    \n",
    "                next_state_idx = discretize_mountaincar_state(next_state, state_bins)\n",
    "\n",
    "            # epsilon greedy\n",
    "            next_action = epsilon_greedy_action(env, Q_table, next_state_idx, epsilon)\n",
    "           \n",
    "            # SARSA update on Q-table\n",
    "            Q_table[state_idx, action] += alpha * (\n",
    "                                        reward + gamma * Q_table[next_state_idx, next_action] - Q_table[state_idx, action]\n",
    "                                    )\n",
    "\n",
    "            state = next_state\n",
    "            state_idx = next_state_idx\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        \n",
    "        # Early stop condition\n",
    "        if(avg_reward>500 and 'CartPole' in env_name) or (avg_reward>-150 and 'MountainCar' in env_name):\n",
    "            avg_length = np.mean(episode_lengths[-100:])\n",
    "            print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Avg Length: {avg_length:.2f}\")\n",
    "            break\n",
    "\n",
    "        if episode % printout == 0: # printout the training progress\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_length = np.mean(episode_lengths[-100:])\n",
    "            print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Avg Length: {avg_length:.2f}\")\n",
    "            \n",
    "\n",
    "    env.close()\n",
    "    return Q_table, episode_rewards, episode_lengths\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620d88a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:00.154216Z",
     "start_time": "2025-11-12T18:42:00.140549Z"
    }
   },
   "outputs": [],
   "source": [
    "# This functions are for visualization of episodes after training\n",
    "# -------------------------\n",
    "# Render Episodes Using RGB Frames\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "\n",
    "def create_gif(frames, filename, fps=5):\n",
    "    \"\"\"Creates a GIF animation from a list of frames.\"\"\"\n",
    "    clip = ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_gif(filename, fps=fps)\n",
    "\n",
    "    \n",
    "    \n",
    "def run_multi_episodes(env_name, Q_table, run_num=10):\n",
    "    \"\"\"Run a single episode using the learned Q-table.\"\"\"\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    \n",
    "    if('CartPole' in env_name):\n",
    "        state_bins = [6, 6, 12, 6]\n",
    "    if('MountainCar' in env_name):\n",
    "        state_bins = [20, 20]\n",
    "    \n",
    "    total_frames = []\n",
    "    total_reward = []\n",
    "    for run in range(run_num):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        if('CartPole' in env_name):\n",
    "            state_idx = discretize_cartpole_state(state, state_bins)\n",
    "        if('MountainCar' in env_name):\n",
    "            state_idx = discretize_mountaincar_state(state, state_bins)\n",
    "            \n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        frames = [env.render()]\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(Q_table[state_idx])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            if('CartPole' in env_name):\n",
    "                next_state_idx = discretize_cartpole_state(next_state, state_bins)\n",
    "            if('MountainCar' in env_name):    \n",
    "                next_state_idx = discretize_mountaincar_state(next_state, state_bins)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            \n",
    "            frames.append(env.render())\n",
    "            episode_reward += reward\n",
    "            state_idx = next_state_idx\n",
    "            \n",
    "        total_frames.extend(frames)\n",
    "        total_reward.append(episode_reward)\n",
    "    return total_frames, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b196d",
   "metadata": {},
   "source": [
    "## Experiment for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b6b2b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:08.885331Z",
     "start_time": "2025-11-12T18:42:00.155013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: 9.00, Avg Length: 9.00\n",
      "Episode 500, Avg Reward: 12.37, Avg Length: 12.37\n",
      "Episode 1000, Avg Reward: 24.21, Avg Length: 24.21\n",
      "Episode 1500, Avg Reward: 27.33, Avg Length: 27.33\n",
      "Episode 2000, Avg Reward: 36.25, Avg Length: 36.25\n",
      "Episode 2500, Avg Reward: 126.74, Avg Length: 126.74\n",
      "Episode 2872, Avg Reward: 516.32, Avg Length: 516.32\n"
     ]
    }
   ],
   "source": [
    "# Continuous states space\n",
    "\n",
    "learned_Q_table, episode_rewards, episode_lengths = Train_SARSA('CartPole-v1', \n",
    "                                                                20000, # total episodes\n",
    "                                                                500,  # printout every 500 episodes\n",
    "                                                                alpha=0.1, \n",
    "                                                                gamma=0.99, \n",
    "                                                                epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59da8a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:09.062047Z",
     "start_time": "2025-11-12T18:42:08.886169Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot return and length for episodes\n",
    "# plots will be saved into the project folder\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Episode Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(episode_lengths)\n",
    "plt.title(\"Episode Lengths\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Number of Steps\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'learning_progress_CartPole.png', bbox_inches='tight',  dpi=100)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d6fc88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:30.884701Z",
     "start_time": "2025-11-12T18:42:09.062825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing an episode ...\n",
      "Total reward: [500.0, 500.0, 500.0]\n",
      "Generating GIF please wait ...\n",
      "MoviePy - Building file CartPole_SARSA.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF is ready!\n"
     ]
    }
   ],
   "source": [
    "print('Playing an episode ...')\n",
    "total_frames, total_reward = run_multi_episodes('CartPole-v1', learned_Q_table, run_num=3)\n",
    "print(f'Total reward: {total_reward}')\n",
    "print('Generating GIF please wait ...')\n",
    "create_gif(total_frames, 'CartPole_SARSA.gif', fps=25)\n",
    "print('GIF is ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8370b4d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Experiment for MountainCar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112d8ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:57.185783Z",
     "start_time": "2025-11-12T18:42:30.885537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: -10761.00, Avg Length: 10761.00\n",
      "Episode 200, Avg Reward: -403.69, Avg Length: 403.69\n",
      "Episode 400, Avg Reward: -299.10, Avg Length: 299.10\n",
      "Episode 600, Avg Reward: -307.94, Avg Length: 307.94\n",
      "Episode 800, Avg Reward: -249.23, Avg Length: 249.23\n",
      "Episode 1000, Avg Reward: -214.53, Avg Length: 214.53\n",
      "Episode 1200, Avg Reward: -235.38, Avg Length: 235.38\n",
      "Episode 1400, Avg Reward: -223.74, Avg Length: 223.74\n",
      "Episode 1600, Avg Reward: -199.89, Avg Length: 199.89\n",
      "Episode 1800, Avg Reward: -184.43, Avg Length: 184.43\n",
      "Episode 2000, Avg Reward: -183.23, Avg Length: 183.23\n",
      "Episode 2200, Avg Reward: -175.72, Avg Length: 175.72\n",
      "Episode 2400, Avg Reward: -180.97, Avg Length: 180.97\n",
      "Episode 2600, Avg Reward: -163.96, Avg Length: 163.96\n",
      "Episode 2800, Avg Reward: -173.84, Avg Length: 173.84\n",
      "Episode 3000, Avg Reward: -220.63, Avg Length: 220.63\n",
      "Episode 3200, Avg Reward: -169.04, Avg Length: 169.04\n",
      "Episode 3400, Avg Reward: -189.41, Avg Length: 189.41\n",
      "Episode 3533, Avg Reward: -149.81, Avg Length: 149.81\n"
     ]
    }
   ],
   "source": [
    "# Continuous states space\n",
    "\n",
    "learned_Q_table, episode_rewards, episode_lengths = Train_SARSA('MountainCar-v0', \n",
    "                                                                20000, # total episodes\n",
    "                                                                200,  # printout every 200 episodes\n",
    "                                                                alpha=0.1, \n",
    "                                                                gamma=0.99, \n",
    "                                                                epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea7f70d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:42:57.342066Z",
     "start_time": "2025-11-12T18:42:57.186666Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot return and length for episodes\n",
    "# plots will be saved into the project folder\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Episode Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(episode_lengths)\n",
    "plt.title(\"Episode Lengths\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Number of Steps\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'learning_progress_MountainCar.png', bbox_inches='tight',  dpi=100)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e589028e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:43:03.654869Z",
     "start_time": "2025-11-12T18:42:57.342893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing an episode ...\n",
      "Total reward: [-161.0, -162.0, -158.0]\n",
      "Generating GIF please wait ...\n",
      "MoviePy - Building file MountainCar_SARSA.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF is ready!\n"
     ]
    }
   ],
   "source": [
    "print('Playing an episode ...')\n",
    "total_frames, total_reward = run_multi_episodes('MountainCar-v0', learned_Q_table, run_num=3)\n",
    "print(f'Total reward: {total_reward}')\n",
    "print('Generating GIF please wait ...')\n",
    "create_gif(total_frames, 'MountainCar_SARSA.gif', fps=25)\n",
    "print('GIF is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd980b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
