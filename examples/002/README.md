# ğŸš– Taxi with Value Iteration ğŸš€

Welcome to the **Taxi** environment, where weâ€™ll guide our little yellow taxi around the city using the **value iteration algorithm**! ğŸŒŸ

Ready to help our driver get the passengers to their destinations? Letâ€™s jump in! ğŸ®

<br><br>

## ğŸ™ï¸ About the Taxi-v3 Environment

In the **Taxi-v3** environment from **Gymnasium**, our goal is to help a taxi driver pick up and drop off passengers at various locations on a grid world. The environment teaches reinforcement learning agents how to make optimal decisions in a discrete, grid-based world.

The taxi moves on a **5Ã—5 grid**, navigating between landmarks to pick up and drop off passengers as efficiently as possible.

---

### ğŸš— States

Each **state** in Taxi-v3 represents a unique combination of:

* The **taxiâ€™s position** (on a 5Ã—5 grid â†’ 25 possible positions).
* The **passengerâ€™s location** (5 possible: ğŸŸ¥ **R**, ğŸŸ© **G**, ğŸŸ¨ **Y**, ğŸŸ¦ **B**, or â€œin taxiâ€).
* The **destination** (4 possible: R, G, Y, or B).

So the **total number of states** is:

```markdown
Total states = 25 (taxi positions) Ã— 5 (passenger locations) Ã— 4 (destinations) = 500 states
```

Each of these 500 states represents a unique configuration of the environment.

---

### ğŸ“Š Space Info

| Space Type                | Description                                                                           |
| ------------------------- | ------------------------------------------------------------------------------------- |
| **Passenger locations**   | 5 (R, G, Y, B, or â€œin taxiâ€)                                                          |
| **Destination locations** | 4 (R, G, Y, B)                                                                        |
| **Observation space**     | 5 rows Ã— 5 columns Ã— 5 passenger locations Ã— 4 destinations = **500 states**          |
| **State representation**  | The environment uses a single integer (**0â€“499**) to represent each state internally. |

---

### ğŸ” Space Decoding

You can decode a state index into its grid representation like this:

```python
import gymnasium as gym

env = gym.make("Taxi-v3")
state = 123
decoded = env.unwrapped.decode(state)
print(decoded)  # (taxi_row, taxi_col, passenger_location, destination)
```

---

### ğŸ§© Space Encoding

You can also encode specific grid positions back into a single integer state:

```python
encoded = env.unwrapped.encode(taxi_row, taxi_col, passenger_loc, destination)
```

---

### ğŸ® Actions

The action space is discrete, with **6 possible actions**:

| Action | Description           |
| ------ | --------------------- |
| **0**  | Move South â¬‡ï¸         |
| **1**  | Move North â¬†ï¸         |
| **2**  | Move East â¡ï¸          |
| **3**  | Move West â¬…ï¸          |
| **4**  | Pick up passenger ğŸš•  |
| **5**  | Drop off passenger ğŸ¯ |

---

### ğŸ’¡ Rewards

| Action Type             | Reward  |
| ----------------------- | ------- |
| Successful drop-off     | **+20** |
| Each time step          | **-1**  |
| Invalid pickup/drop-off | **-10** |

A successful episode means picking up and dropping off a passenger correctly and efficiently to maximize cumulative reward. ğŸ…

---

### ğŸ”„ Transition Probabilities

In reinforcement learning, the **transition probabilities** define how likely it is to move from one state to another after taking a specific action.

In Taxi-v3, these are accessible via:

```python
env.unwrapped.P[s][a]
```

Each entry gives a list of tuples `(probability, next_state, reward, terminated)` for all possible outcomes from state `s` with action `a`.

Example:

```python
env.unwrapped.P[1][4]
```

returns all transitions from state **1** after taking **action 4 (pickup)**.

These probabilities are essential for computing expected returns in **value iteration**.

---

## ğŸ§  Value Iteration Algorithm

The **Value Iteration** algorithm is a dynamic programming approach used to find the optimal value function and policy in Markov Decision Processes (MDPs).
It works by repeatedly updating the estimated value of each state based on the **Bellman optimality equation** until convergence.

---

### ğŸ§¾ Algorithm: Value Iteration

```markdown
**Input:** 
  - Environment with state set S and action set A
  - Transition probabilities P(s' | s, a)
  - Rewards R(s, a, s')
  - Discount factor Î³ âˆˆ [0, 1)
  - Convergence threshold Î¸

**Output:** 
  - Optimal state-value function V*(s)
  - Optimal policy Ï€*(s)

**Algorithm Steps:**

1. Initialize V(s) = 0 for all s âˆˆ S

2. Repeat until convergence:
     Î” â† 0
     For each state s âˆˆ S:
         a. For each action a âˆˆ A(s):
              Compute expected return:
              Q(s, a) = Î£ [ P(s'|s, a) * (R(s, a, s') + Î³ * V(s')) ] over all s'
         b. Update state value:
              V_new(s) = max(Q(s, a)) for all actions a
         c. Update change magnitude:
              Î” = max(Î”, |V_new(s) - V(s)|)
         d. Set V(s) â† V_new(s)
     If Î” < Î¸, then stop (converged)

3. Derive the optimal policy:
     For each state s âˆˆ S:
         Ï€*(s) = argmax_a [ Î£ P(s'|s, a) * (R(s, a, s') + Î³ * V(s')) ]

4. Return V*(s), Ï€*(s)
```

---

### ğŸ§­ Intuition

* **Step 1:** Start with arbitrary state values (usually zeros).
* **Step 2:** Iteratively update each state value based on expected rewards plus discounted future values.
* **Step 3:** Stop when changes become negligible (i.e., the values have â€œconvergedâ€).
* **Step 4:** Derive the **optimal policy** that picks the best action in every state.

---




## ğŸ› ï¸ Letâ€™s Set Up Taxi-v3! ğŸš•

Before we start driving around with **Taxi-v3**, letâ€™s get everything ready! ğŸ™Œ
Youâ€™ll need the **uv** package and project manager installed to make sure your environment is clean, reproducible, and easy to manage. ğŸ§°

You can grab **uv** from its [GitHub repository](https://github.com/astral-sh/uv), or just run this one-liner to install it:

```bash
# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Now, why are we using **uv**? ğŸ¤”
Because itâ€™s our pit crew! ğŸï¸ It ensures that every project has the right Python setup and dependencies â€” no conflicts, no mess.
This way, youâ€™ll be able to run **Taxi-v3** smoothly without any detours. ğŸ¯

---

### ğŸ“¦ Initialize the Example:

Letâ€™s create a clean environment for our Taxi project. ğŸš¦

```bash
# Pin Python version
uv python pin 3.12

# Initialize a new project
uv init 002 && cd 002
# we are in examples/002
# Remove the default main.py if created
rm main.py

# Create a virtual environment to ensure version consistency
uv venv --python 3.12 # just for sure
```

Almost ready! But before we hit the road, we need to install a few dependencies. ğŸ§©

---

### ğŸ“¥ Install Project Dependencies:

Time to install the necessary packages for the experiment so everything works as expected! ğŸ’»âœ¨

Make sure you're in the **examples/002/** directory and run:


```bash
# you must be in path examples/002/
uv add -r requirements.txt
```

---

### ğŸš€ Launch Jupyter Notebook:

Now that everythingâ€™s set up, letâ€™s start our engines and open Jupyter to begin exploring the **Taxi-v3** world! ğŸ®

```bash
uv run jupyter notebook --ip='*' --NotebookApp.token='' --NotebookApp.password=''
```

There you go! Everythingâ€™s ready for you to start training and testing your **Taxi** agent. ğŸš•

With **Value Iteration**, your agent will discover the **optimal strategy** â€” moving efficiently, avoiding penalties, and maximizing rewards.

Letâ€™s make this taxi the most efficient cab in the city! ğŸ’›âœ¨

---

