
<p align="center">
  <img src="./trained_Taxi-v3_1.gif" alt="Taxi Agent in Action" width="300"/><br>
  <em>The Taxi-v3 agent efficiently navigating the grid world using Backward-View SARSA(Î»)!</em>
</p>

<p align="center">
  <img src="./trained_CliffWalking-v1_eps_09.gif" alt="Cliff Agent in Action" width="400"/><br>
  <em>Cliff Walking trained using Backward-View SARSA(Î») â€” smoother and more stable learning!</em>
</p>

<br><br>

# ğŸš• Taxi-v3 & CliffWalking-v1 with **Backward-View SARSA(Î»)** (Stateâ€“Actionâ€“Rewardâ€“Stateâ€“Action with Eligibility Traces) ğŸ¯

Welcome to the **Taxi-v3** and **CliffWalking-v1** environments!
In this project, we train an agent to learn **optimal navigation policies** using the **Backward-View SARSA(Î»)** algorithm â€” an enhanced version of SARSA that combines **Temporal-Difference (TD) learning** with **eligibility traces** to achieve **faster and more stable learning**. âš¡

Backward-View SARSA(Î») is a **model-free, on-policy TD control algorithm**. It improves upon classic SARSA by allowing learning signals (TD errors) to **propagate backward in time**, giving credit not only to the most recent stateâ€“action pair but also to **all past pairs** that led to it. This makes the algorithm **more data-efficient** and helps the agent **learn smoother, longer-term behaviors**.

---

## ğŸ§  **Backward-View SARSA(Î»): A Brief Overview**

**Backward-View SARSA(Î»)** extends the traditional SARSA algorithm by introducing **eligibility traces**, which act as a **short-term memory** of recently visited stateâ€“action pairs.

In standard SARSA, updates occur **only for the most recent** stateâ€“action pair:

```
Q(s, a) â† Q(s, a) + Î± [r + Î³ Q(s', a') âˆ’ Q(s, a)]
```

In contrast, **Backward-View SARSA(Î»)** updates **all previously visited** pairs in proportion to their eligibility, defined by:

```
E(s, a) â† Î³Î»E(s, a)
```

and applies the update:

```
Q(s, a) â† Q(s, a) + Î± Î´ E(s, a)
```

where Î´ (the TD error) is:

```
Î´ = r + Î³ Q(s', a') âˆ’ Q(s, a)
```

---

### âš¡ **Why Backward-View SARSA(Î») is Better**

| Feature                  | Standard SARSA                    | Backward-View SARSA(Î»)                                           |
| ------------------------ | --------------------------------- | ---------------------------------------------------------------- |
| **Credit Assignment**    | Updates only the last (s, a) pair | Propagates learning signal backward to many recent (s, a) pairs  |
| **Learning Speed**       | Slower, local updates             | Faster convergence due to broader updates                        |
| **Stability**            | Can fluctuate on noisy rewards    | Smoother and more stable due to averaging effect of traces       |
| **Memory of Experience** | None                              | Keeps a short-term "memory" of recent steps (eligibility traces) |
| **Performance**          | Good on short tasks               | Superior on long sequences (e.g., CliffWalking)                  |

---

### ğŸ§© **Intuition**

Think of eligibility traces as a **fade-out memory**:
recent stateâ€“action pairs are given high credit, while older ones gradually fade.
This allows the agent to efficiently **bridge the gap between cause and effect**, learning from delayed rewards much faster than plain SARSA.

---

## ğŸ§­ **Backward-View SARSA(Î») in Taxi-v3 & CliffWalking-v1**

* **Taxi-v3** ğŸš–
  The agent learns to pick up and drop off passengers efficiently.
  Backward-View SARSA(Î») helps the taxi **remember** sequences of moves that lead to successful drop-offs, improving learning speed and stability.

* **CliffWalking-v1** ğŸ§—
  The agent learns to cross the grid without falling off the cliff.
  Backward-View SARSA(Î») gives credit to earlier safe moves, enabling **smoother navigation** and reducing oscillations near the cliff edge.

---

## ğŸš– About the Taxi-v3 Environment

**Taxi-v3** from **Gymnasium** simulates a taxi navigating a grid world to **pick up and drop off passengers** at designated locations.

### ğŸ¯ Objective

Pick up the passenger and deliver them with the **fewest possible steps** while avoiding penalties.

| Space Type            | Description                                                             |
| --------------------- | ----------------------------------------------------------------------- |
| **Observation space** | 500 discrete states (taxi row, column, passenger location, destination) |
| **Action space**      | 6 discrete actions                                                      |

| Action | Description   |
| ------ | ------------- |
| 0      | Move South â¬‡ï¸ |
| 1      | Move North â¬†ï¸ |
| 2      | Move East â¡ï¸  |
| 3      | Move West â¬…ï¸  |
| 4      | Pick Up ğŸš–    |
| 5      | Drop Off ğŸ¯   |

| Event               | Reward  |
| ------------------- | ------- |
| Successful drop-off | **+20** |
| Each timestep       | **-1**  |
| Illegal pickup/drop | **-10** |

---

## ğŸ§— About the CliffWalking-v1 Environment

**CliffWalking-v1** represents an agent navigating a grid while avoiding a cliff with heavy penalties.

| Space Type            | Description                    |
| --------------------- | ------------------------------ |
| **Observation space** | 48 discrete states (4Ã—12 grid) |
| **Action space**      | 4 discrete actions             |

| Action | Description   |
| ------ | ------------- |
| 0      | Move Left â¬…ï¸  |
| 1      | Move Down â¬‡ï¸  |
| 2      | Move Right â¡ï¸ |
| 3      | Move Up â¬†ï¸    |

| Event          | Reward   |
| -------------- | -------- |
| Reach Goal     | **0**    |
| Step Normally  | **-1**   |
| Fall off Cliff | **-100** |

Backward-View SARSA(Î») enables the agent to learn **safer and more direct paths**, crediting sequences of steps that avoided the cliff.

---

## ğŸ§¾ **Algorithm: Backward-View SARSA(Î»)**

**Input:**

* Environment with state set **S** and action set **A**
* Learning rate **Î±**
* Discount factor **Î³**
* Trace decay rate **Î»**
* Exploration rate **Îµ**
* Number of episodes **N**

**Output:**

* Optimal action-value function **Q*(s, a)**
* Policy **Ï€*(s)**

**Steps:**

1. Initialize **Q(s, a)** and **E(s, a)** (eligibility traces) to 0.

2. For each episode:

   * Initialize state **s** and action **a** (Îµ-greedy).
   * Repeat for each step:

     1. Take action **a**, observe **r** and **sâ€²**.
     2. Choose next action **aâ€²** (Îµ-greedy).
     3. Compute TD error: `Î´ = r + Î³ Q(sâ€², aâ€²) âˆ’ Q(s, a)`
     4. Increment eligibility: `E(s, a) â† E(s, a) + 1`
     5. Update all Q-values: `Q â† Q + Î± Î´ E`
     6. Decay traces: `E â† Î³Î»E`
     7. Set **s â† sâ€²**, **a â† aâ€²** until episode ends.

3. Derive final policy: **Ï€*(s) = argmaxâ‚ Q(s, a)**

---

## ğŸ§  **Key Advantages of Backward-View SARSA(Î»)**

 âœ… **Faster convergence** â€“ propagates learning signals across multiple time steps.

 âœ… **Better credit assignment** â€“ rewards influence earlier decisions that contributed to success.

 âœ… **Improved stability** â€“ smooths out updates, especially in noisy or long-horizon environments.

 âœ… **Generalizes classic SARSA** â€“ setting Î» = 0 recovers the standard SARSA algorithm.

---

## ğŸ› ï¸ Letâ€™s Set Up the Project! ğŸš€

```bash
# Install uv package
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### ğŸ“¦ Initialize the Example

```bash
cd examples/008
uv python pin 3.12
uv init
rm main.py
uv venv --python 3.12
```

### ğŸ“¥ Install Project Dependencies

```bash
uv add -r requirements.txt # we are in examples/008
```

### ğŸš€ Launch Jupyter Notebook

```bash
uv run jupyter notebook --ip='*' --NotebookApp.token='' --NotebookApp.password=''
```

Your **Backward-View SARSA(Î») agent** is now ready to learn optimal, memory-augmented navigation in **Taxi-v3** and **CliffWalking-v1**! ğŸ§ âš¡
Watch as it learns **faster, smoother, and smarter** than ever before.

---

