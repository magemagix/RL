{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1537e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:24:58.759339Z",
     "start_time": "2025-11-11T18:24:58.494971Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict  # required for creating Q(s, a)\n",
    "from moviepy import ImageSequenceClip # to generate gif\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib \n",
    "#matplotlib.use('Qt5Agg') # Activte it if you want external plot for any interaction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36689210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:24:59.432337Z",
     "start_time": "2025-11-11T18:24:59.430752Z"
    }
   },
   "outputs": [],
   "source": [
    "# We train our agent by SARSA in two following environments\n",
    "\n",
    "envs = ['Taxi-v3', 'CliffWalking-v1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50524f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:24:59.976688Z",
     "start_time": "2025-11-11T18:24:59.973977Z"
    }
   },
   "outputs": [],
   "source": [
    "# This functions are for visualization of episodes after training\n",
    "# -------------------------\n",
    "# Render Episodes Using RGB Frames\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "\n",
    "def create_gif(frames, filename, fps=5):\n",
    "    \"\"\"Creates a GIF animation from a list of frames.\"\"\"\n",
    "    clip = ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_gif(filename, fps=fps)\n",
    "\n",
    "    \n",
    "    \n",
    "def run_multi_episodes(env, Q_table, run_num=10, epsilon=0):\n",
    "    \"\"\"Run a single episode using the learned Q-table.\"\"\"\n",
    "    total_frames = []\n",
    "    total_reward = []\n",
    "    for run in range(run_num):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        frames = [env.render()]\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(Q_table[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            frames.append(env.render())\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        total_frames.extend(frames)\n",
    "        total_reward.append(episode_reward)\n",
    "    return total_frames, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bdd3d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:25:00.581983Z",
     "start_time": "2025-11-11T18:25:00.567992Z"
    }
   },
   "outputs": [],
   "source": [
    "def Train_SARSA(envIdx, episodes_num, printout, output_name, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    \n",
    "    # Set up the environment\n",
    "    env = gym.make(envs[envIdx])\n",
    "    \n",
    "    \n",
    "    # Initializing the Q table\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q_table = np.zeros((n_states, n_actions))\n",
    "    print(f'Environment {envs[envIdx]}:\\n    Size of observation space:{n_states}\\n    Size of action space:{n_actions}')\n",
    "    \n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    \n",
    "    for episode in range(episodes_num):\n",
    "        \n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])\n",
    "            \n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # epsilon greedy\n",
    "            if np.random.random() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = np.argmax(Q_table[next_state])\n",
    "\n",
    "            # SARSA update on Q-table\n",
    "            Q_table[state, action] += alpha * (\n",
    "                                        reward + gamma * Q_table[next_state, next_action] - Q_table[state, action]\n",
    "                                    )\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        epsilon *= 0.99 \n",
    "\n",
    "        if episode % printout == 0: # printout the training progress\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_length = np.mean(episode_lengths[-100:])\n",
    "            print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Avg Length: {avg_length:.2f}\")\n",
    "            \n",
    "\n",
    "            \n",
    "    \n",
    "    # plot return and length for episodes\n",
    "    # plots will be saved into the project folder\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.title(\"Episode Return\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(episode_lengths)\n",
    "    plt.title(\"Episode Lengths\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Number of Steps\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'metrics_{envs[envIdx]}_{output_name}.png', bbox_inches='tight',  dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # play and visualize some episodes\n",
    "    env_vis = gym.make(envs[envIdx], render_mode='rgb_array')\n",
    "    frames, total_reward = run_multi_episodes(env_vis, Q_table, run_num=5)\n",
    "    create_gif(frames, f\"trained_{envs[envIdx]}_{output_name}.gif\", fps=5)\n",
    "    print(f\"Episodes completed with total rewards: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab23c834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:25:05.677457Z",
     "start_time": "2025-11-11T18:25:01.256428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Taxi-v3:\n",
      "    Size of observation space:500\n",
      "    Size of action space:6\n",
      "Episode 0, Avg Reward: -659.00, Avg Length: 200.00\n",
      "Episode 2500, Avg Reward: 8.49, Avg Length: 12.51\n",
      "Episode 5000, Avg Reward: 8.32, Avg Length: 12.68\n",
      "Episode 7500, Avg Reward: 7.85, Avg Length: 13.15\n",
      "Episode 10000, Avg Reward: 8.01, Avg Length: 12.99\n",
      "Episode 12500, Avg Reward: 8.01, Avg Length: 12.99\n",
      "Episode 15000, Avg Reward: 7.94, Avg Length: 13.06\n",
      "Episode 17500, Avg Reward: 7.97, Avg Length: 13.03\n",
      "MoviePy - Building file trained_Taxi-v3_1.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes completed with total rewards: [7, 8, 7, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "# training the agent and visualizing some episodes for Taxi-v3 env\n",
    "\n",
    "Train_SARSA(0, 20000, 2500, '1', alpha=0.1, gamma=0.99, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7029dee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:25:08.831519Z",
     "start_time": "2025-11-11T18:25:07.880237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment CliffWalking-v1:\n",
      "    Size of observation space:48\n",
      "    Size of action space:4\n",
      "Episode 0, Avg Reward: -129.00, Avg Length: 129.00\n",
      "Episode 50, Avg Reward: -184.53, Avg Length: 114.65\n",
      "Episode 100, Avg Reward: -123.55, Avg Length: 79.99\n",
      "Episode 150, Avg Reward: -49.16, Avg Length: 38.27\n",
      "Episode 200, Avg Reward: -32.49, Avg Length: 27.54\n",
      "Episode 250, Avg Reward: -26.86, Avg Length: 22.90\n",
      "Episode 300, Avg Reward: -22.11, Avg Length: 20.13\n",
      "Episode 350, Avg Reward: -18.00, Avg Length: 18.00\n",
      "Episode 400, Avg Reward: -16.81, Avg Length: 16.81\n",
      "Episode 450, Avg Reward: -15.80, Avg Length: 15.80\n",
      "MoviePy - Building file trained_CliffWalking-v1_eps_01.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes completed with total rewards: [-15, -15, -15, -15, -15]\n"
     ]
    }
   ],
   "source": [
    "# training the agent and visualizing some episodes for CliffWalking-v1 env\n",
    "# limit the exploration by 0.1\n",
    "Train_SARSA(1, 500, 50, 'eps_01', alpha=0.1, gamma=0.99, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710a868d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:25:11.448498Z",
     "start_time": "2025-11-11T18:25:10.327016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment CliffWalking-v1:\n",
      "    Size of observation space:48\n",
      "    Size of action space:4\n",
      "Episode 0, Avg Reward: -19307.00, Avg Length: 2873.00\n",
      "Episode 50, Avg Reward: -1749.22, Avg Length: 316.63\n",
      "Episode 100, Avg Reward: -738.29, Avg Length: 158.15\n",
      "Episode 150, Avg Reward: -57.00, Avg Length: 37.20\n",
      "Episode 200, Avg Reward: -29.54, Avg Length: 21.62\n",
      "Episode 250, Avg Reward: -21.03, Avg Length: 19.05\n",
      "Episode 300, Avg Reward: -19.31, Avg Length: 18.32\n",
      "Episode 350, Avg Reward: -18.82, Avg Length: 17.83\n",
      "Episode 400, Avg Reward: -17.39, Avg Length: 17.39\n",
      "Episode 450, Avg Reward: -17.27, Avg Length: 17.27\n",
      "MoviePy - Building file trained_CliffWalking-v1_eps_09.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes completed with total rewards: [-17, -17, -17, -17, -17]\n"
     ]
    }
   ],
   "source": [
    "# training the agent and visualizing some episodes for CliffWalking-v1 env\n",
    "# allow the exploration by 0.9\n",
    "Train_SARSA(1, 500, 50, 'eps_09', alpha=0.1, gamma=0.99, epsilon=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e97a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
