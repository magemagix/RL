<p align="center">
  <img src="./Taxi-policy-iteration.gif" alt="Taxi Agent in Action" width="500"/><br>
  <em>The Taxi agent efficiently navigating the 5Ã—5 grid!</em>
</p>

<br><br>




# ğŸš– Taxi with Policy Iteration ğŸš€

Welcome to the **Taxi** environment, where weâ€™ll guide our little yellow taxi around the city using the **policy iteration algorithm**! ğŸŒŸ

Ready to help our driver get the passengers to their destinations? Letâ€™s jump in! ğŸ®

<br><br>

## ğŸ™ï¸ About the Taxi-v3 Environment

In the **Taxi-v3** environment from **Gymnasium**, our goal is to help a taxi driver pick up and drop off passengers at various locations on a grid world. The environment teaches reinforcement learning agents how to make optimal decisions in a discrete, grid-based world.

The taxi moves on a **5Ã—5 grid**, navigating between landmarks to pick up and drop off passengers as efficiently as possible.

---

### ğŸš— States

Each **state** in Taxi-v3 represents a unique combination of:

* The **taxiâ€™s position** (on a 5Ã—5 grid â†’ 25 possible positions).
* The **passengerâ€™s location** (5 possible: ğŸŸ¥ **R**, ğŸŸ© **G**, ğŸŸ¨ **Y**, ğŸŸ¦ **B**, or â€œin taxiâ€).
* The **destination** (4 possible: R, G, Y, or B).

So the **total number of states** is:

```markdown
Total states = 25 (taxi positions) Ã— 5 (passenger locations) Ã— 4 (destinations) = 500 states
```

Each of these 500 states represents a unique configuration of the environment.

---

### ğŸ“Š Space Info

| Space Type                | Description                                                                           |
| ------------------------- | ------------------------------------------------------------------------------------- |
| **Passenger locations**   | 5 (R, G, Y, B, or â€œin taxiâ€)                                                          |
| **Destination locations** | 4 (R, G, Y, B)                                                                        |
| **Observation space**     | 5 rows Ã— 5 columns Ã— 5 passenger locations Ã— 4 destinations = **500 states**          |
| **State representation**  | The environment uses a single integer (**0â€“499**) to represent each state internally. |

---

### ğŸ” Space Decoding

You can decode a state index into its grid representation like this:

```python
import gymnasium as gym

env = gym.make("Taxi-v3")
state = 123
decoded = env.unwrapped.decode(state)
print(decoded)  # (taxi_row, taxi_col, passenger_location, destination)
```

---

### ğŸ§© Space Encoding

You can also encode specific grid positions back into a single integer state:

```python
encoded = env.unwrapped.encode(taxi_row, taxi_col, passenger_loc, destination)
```

---

### ğŸ® Actions

The action space is discrete, with **6 possible actions**:

| Action | Description           |
| ------ | --------------------- |
| **0**  | Move South â¬‡ï¸         |
| **1**  | Move North â¬†ï¸         |
| **2**  | Move East â¡ï¸          |
| **3**  | Move West â¬…ï¸          |
| **4**  | Pick up passenger ğŸš•  |
| **5**  | Drop off passenger ğŸ¯ |

---

### ğŸ’¡ Rewards

| Action Type             | Reward  |
| ----------------------- | ------- |
| Successful drop-off     | **+20** |
| Each time step          | **-1**  |
| Invalid pickup/drop-off | **-10** |

A successful episode means picking up and dropping off a passenger correctly and efficiently to maximize cumulative reward. ğŸ…

---

### ğŸ”„ Transition Probabilities

In reinforcement learning, the **transition probabilities** define how likely it is to move from one state to another after taking a specific action.

In Taxi-v3, these are accessible via:

```python
env.unwrapped.P[s][a]
```

Each entry gives a list of tuples `(probability, next_state, reward, terminated)` for all possible outcomes from state `s` with action `a`.

Example:

```python
env.unwrapped.P[1][4]
```

returns all transitions from state **1** after taking **action 4 (pickup)**.

These probabilities are essential for computing expected returns in **value iteration**.

---










## ğŸ› ï¸ Letâ€™s Set Up Taxi-v3! ğŸš•

Before we start driving around with **Taxi-v3**, letâ€™s get everything ready! ğŸ™Œ
Youâ€™ll need the **uv** package and project manager installed to make sure your environment is clean, reproducible, and easy to manage. ğŸ§°

You can grab **uv** from its [GitHub repository](https://github.com/astral-sh/uv), or just run this one-liner to install it:

```bash
# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Now, why are we using **uv**? ğŸ¤”
Because itâ€™s our pit crew! ğŸï¸ It ensures that every project has the right Python setup and dependencies â€” no conflicts, no mess.
This way, youâ€™ll be able to run **Taxi-v3** smoothly without any detours. ğŸ¯

---

### ğŸ“¦ Initialize the Example:

Letâ€™s create a clean environment for our Taxi project. ğŸš¦

```bash
# Pin Python version
uv python pin 3.12

# Initialize a new project
uv init 003 && cd 003
# we are in examples/003
# Remove the default main.py if created
rm main.py

# Create a virtual environment to ensure version consistency
uv venv --python 3.12 # just for sure
```

Almost ready! But before we hit the road, we need to install a few dependencies. ğŸ§©

---

### ğŸ“¥ Install Project Dependencies:

Time to install the necessary packages for the experiment so everything works as expected! ğŸ’»âœ¨

Make sure you're in the **examples/003/** directory and run:


```bash
# you must be in path examples/003/
uv add -r requirements.txt
```

---

### ğŸš€ Launch Jupyter Notebook:

Now that everythingâ€™s set up, letâ€™s start our engines and open Jupyter to begin exploring the **Taxi-v3** world! ğŸ®

```bash
uv run jupyter notebook --ip='*' --NotebookApp.token='' --NotebookApp.password=''
```

There you go! Everythingâ€™s ready for you to start training and testing your **Taxi** agent. ğŸš•

With **Policy Iteration**, your agent will discover the **optimal strategy** â€” moving efficiently, avoiding penalties, and maximizing rewards.

Letâ€™s make this taxi the most efficient cab in the city! ğŸ’›âœ¨

---
