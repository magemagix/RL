<p align="center">
  <img src="./Taxi-policy-iteration.gif" alt="Taxi Agent in Action" width="500"/><br>
  <em>The Taxi agent efficiently navigating the 5Ã—5 grid!</em>
</p>

<br><br>




# ğŸš– Taxi with Policy Iteration ğŸš€

Welcome to the **Taxi** environment, where weâ€™ll guide our little yellow taxi around the city using the **policy iteration algorithm**! ğŸŒŸ

Ready to help our driver get the passengers to their destinations? Letâ€™s jump in! ğŸ®

<br><br>

## ğŸ™ï¸ About the Taxi-v3 Environment

In the **Taxi-v3** environment from **Gymnasium**, our goal is to help a taxi driver pick up and drop off passengers at various locations on a grid world. The environment teaches reinforcement learning agents how to make optimal decisions in a discrete, grid-based world.

The taxi moves on a **5Ã—5 grid**, navigating between landmarks to pick up and drop off passengers as efficiently as possible.

---

### ğŸš— States

Each **state** in Taxi-v3 represents a unique combination of:

* The **taxiâ€™s position** (on a 5Ã—5 grid â†’ 25 possible positions).
* The **passengerâ€™s location** (5 possible: ğŸŸ¥ **R**, ğŸŸ© **G**, ğŸŸ¨ **Y**, ğŸŸ¦ **B**, or â€œin taxiâ€).
* The **destination** (4 possible: R, G, Y, or B).

So the **total number of states** is:

```markdown
Total states = 25 (taxi positions) Ã— 5 (passenger locations) Ã— 4 (destinations) = 500 states
```

Each of these 500 states represents a unique configuration of the environment.

---

### ğŸ“Š Space Info

| Space Type                | Description                                                                           |
| ------------------------- | ------------------------------------------------------------------------------------- |
| **Passenger locations**   | 5 (R, G, Y, B, or â€œin taxiâ€)                                                          |
| **Destination locations** | 4 (R, G, Y, B)                                                                        |
| **Observation space**     | 5 rows Ã— 5 columns Ã— 5 passenger locations Ã— 4 destinations = **500 states**          |
| **State representation**  | The environment uses a single integer (**0â€“499**) to represent each state internally. |

---

### ğŸ” Space Decoding

You can decode a state index into its grid representation like this:

```python
import gymnasium as gym

env = gym.make("Taxi-v3")
state = 123
decoded = env.unwrapped.decode(state)
print(decoded)  # (taxi_row, taxi_col, passenger_location, destination)
```

---

### ğŸ§© Space Encoding

You can also encode specific grid positions back into a single integer state:

```python
encoded = env.unwrapped.encode(taxi_row, taxi_col, passenger_loc, destination)
```

---

### ğŸ® Actions

The action space is discrete, with **6 possible actions**:

| Action | Description           |
| ------ | --------------------- |
| **0**  | Move South â¬‡ï¸         |
| **1**  | Move North â¬†ï¸         |
| **2**  | Move East â¡ï¸          |
| **3**  | Move West â¬…ï¸          |
| **4**  | Pick up passenger ğŸš•  |
| **5**  | Drop off passenger ğŸ¯ |

---

### ğŸ’¡ Rewards

| Action Type             | Reward  |
| ----------------------- | ------- |
| Successful drop-off     | **+20** |
| Each time step          | **-1**  |
| Invalid pickup/drop-off | **-10** |

A successful episode means picking up and dropping off a passenger correctly and efficiently to maximize cumulative reward. ğŸ…

---

### ğŸ”„ Transition Probabilities

In reinforcement learning, the **transition probabilities** define how likely it is to move from one state to another after taking a specific action.

In Taxi-v3, these are accessible via:

```python
env.unwrapped.P[s][a]
```

Each entry gives a list of tuples `(probability, next_state, reward, terminated)` for all possible outcomes from state `s` with action `a`.

Example:

```python
env.unwrapped.P[1][4]
```

returns all transitions from state **1** after taking **action 4 (pickup)**.

These probabilities are essential for computing expected returns in **value iteration**.

---


## ğŸ§  Policy Iteration Algorithm

The **Policy Iteration** algorithm is a dynamic programming approach used to find the optimal policy in Markov Decision Processes (MDPs). It works by alternating between **policy evaluation** (calculating the value function for a fixed policy) and **policy improvement** (updating the policy based on the current value function) until convergence.

---

### ğŸ§¾ Algorithm: Policy Iteration

```markdown
**Input:** 
  - Environment with state set S and action set A
  - Transition probabilities P(s' | s, a)
  - Rewards R(s, a, s')
  - Discount factor Î³ âˆˆ [0, 1)
  - Convergence threshold Î¸

**Output:** 
  - Optimal policy Ï€*(s)
  - Optimal state-value function V*(s)

**Algorithm Steps:**

1. Initialize policy Ï€(s) arbitrarily for all s âˆˆ S
   (e.g., random or random actions)

2. Repeat until convergence:
   
   a. **Policy Evaluation**:
      - Initialize V(s) = 0 for all s âˆˆ S
      - Repeat until convergence:
         Î” â† 0
         For each state s âˆˆ S:
             - Compute value function for the current policy Ï€(s):
               V_new(s) = Î£ [ P(s'|s, Ï€(s)) * (R(s, Ï€(s), s') + Î³ * V(s')) ] over all s'
             - Update change magnitude:
               Î” = max(Î”, |V_new(s) - V(s)|)
             - Set V(s) â† V_new(s)
         If Î” < Î¸, then stop (converged)

   b. **Policy Improvement**:
      - For each state s âˆˆ S:
          - Update policy based on the current value function:
            Ï€_new(s) = argmax_a [ Î£ P(s'|s, a) * (R(s, a, s') + Î³ * V(s')) ]
      - If Ï€_new = Ï€, then stop (policy is stable)
      - Otherwise, set Ï€ â† Ï€_new (update policy)

3. Return Ï€*(s), V*(s)
```

---

### ğŸ§­ Intuition

* **Step 1:** Start with an arbitrary initial policy (e.g., random).
* **Step 2:**
  * **Policy Evaluation**: Compute the value of each state under the current policy by solving the Bellman equations.
  * **Policy Improvement**: Improve the policy by choosing the action that maximizes the expected return based on the current value function.
* **Step 3:** Repeat *step 2* until the policy stabilizes (i.e., no further improvements can be made) or reaches the maximum iteration. If policy stabilizes, it means that the optimal policy has been found.
* **Step 4:** Return the **optimal policy** and the corresponding value function.

---

This approach ensures convergence to the optimal policy, though it may require multiple iterations of policy evaluation and improvement.



## ğŸ› ï¸ Letâ€™s Set Up Taxi-v3! ğŸš•

Before we start driving around with **Taxi-v3**, letâ€™s get everything ready! ğŸ™Œ
Youâ€™ll need the **uv** package and project manager installed to make sure your environment is clean, reproducible, and easy to manage. ğŸ§°

You can grab **uv** from its [GitHub repository](https://github.com/astral-sh/uv), or just run this one-liner to install it:

```bash
# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Now, why are we using **uv**? ğŸ¤”
Because itâ€™s our pit crew! ğŸï¸ It ensures that every project has the right Python setup and dependencies â€” no conflicts, no mess.
This way, youâ€™ll be able to run **Taxi-v3** smoothly without any detours. ğŸ¯

---

### ğŸ“¦ Initialize the Example:

Letâ€™s create a clean environment for our Taxi project. ğŸš¦

```bash
# we need to be in examples/003
cd examples/003

# Pin Python version
uv python pin 3.12

# Initialize a new project
uv init
# we are in examples/003
# Remove the default main.py if created
rm main.py

# Create a virtual environment to ensure version consistency
uv venv --python 3.12 # just for sure
```

Almost ready! But before we hit the road, we need to install a few dependencies. ğŸ§©

---

### ğŸ“¥ Install Project Dependencies:

Time to install the necessary packages for the experiment so everything works as expected! ğŸ’»âœ¨

Make sure you're in the **examples/003/** directory and run:


```bash
# you must be in path examples/003/
uv add -r requirements.txt
```

---

### ğŸš€ Launch Jupyter Notebook:

Now that everythingâ€™s set up, letâ€™s start our engines and open Jupyter to begin exploring the **Taxi-v3** world! ğŸ®

```bash
uv run jupyter notebook --ip='*' --NotebookApp.token='' --NotebookApp.password=''
```

There you go! Everythingâ€™s ready for you to start training and testing your **Taxi** agent. ğŸš•

With **Policy Iteration**, your agent will discover the **optimal strategy** â€” moving efficiently, avoiding penalties, and maximizing rewards.

Letâ€™s make this taxi the most efficient cab in the city! ğŸ’›âœ¨

---
