<p align="center">
  <img src="./Taxi-policy-iteration.gif" alt="Taxi Agent in Action" width="500"/><br>
  <em>The Taxi agent efficiently navigating the 5Ã—5 grid!</em>
</p>

<br><br>




# ğŸš– Taxi with Policy Iteration ğŸš€

Welcome to the **Taxi** environment, where weâ€™ll guide our little yellow taxi around the city using the **policy iteration algorithm**! ğŸŒŸ

Ready to help our driver get the passengers to their destinations? Letâ€™s jump in! ğŸ®

<br><br>

## ğŸ™ï¸ About the Taxi-v3 Environment

In the **Taxi-v3** environment from **Gymnasium**, our goal is to help a taxi driver pick up and drop off passengers at various locations on a grid world. The environment teaches reinforcement learning agents how to make optimal decisions in a discrete, grid-based world.

The taxi moves on a **5Ã—5 grid**, navigating between landmarks to pick up and drop off passengers as efficiently as possible.

---

### ğŸš— States

Each **state** in Taxi-v3 represents a unique combination of:

* The **taxiâ€™s position** (on a 5Ã—5 grid â†’ 25 possible positions).
* The **passengerâ€™s location** (5 possible: ğŸŸ¥ **R**, ğŸŸ© **G**, ğŸŸ¨ **Y**, ğŸŸ¦ **B**, or â€œin taxiâ€).
* The **destination** (4 possible: R, G, Y, or B).

So the **total number of states** is:

```markdown
Total states = 25 (taxi positions) Ã— 5 (passenger locations) Ã— 4 (destinations) = 500 states
```

Each of these 500 states represents a unique configuration of the environment.

---

### ğŸ“Š Space Info

| Space Type                | Description                                                                           |
| ------------------------- | ------------------------------------------------------------------------------------- |
| **Passenger locations**   | 5 (R, G, Y, B, or â€œin taxiâ€)                                                          |
| **Destination locations** | 4 (R, G, Y, B)                                                                        |
| **Observation space**     | 5 rows Ã— 5 columns Ã— 5 passenger locations Ã— 4 destinations = **500 states**          |
| **State representation**  | The environment uses a single integer (**0â€“499**) to represent each state internally. |

---

### ğŸ” Space Decoding

You can decode a state index into its grid representation like this:

```python
import gymnasium as gym

env = gym.make("Taxi-v3")
state = 123
decoded = env.unwrapped.decode(state)
print(decoded)  # (taxi_row, taxi_col, passenger_location, destination)
```

---

### ğŸ§© Space Encoding

You can also encode specific grid positions back into a single integer state:

```python
encoded = env.unwrapped.encode(taxi_row, taxi_col, passenger_loc, destination)
```

---

### ğŸ® Actions

The action space is discrete, with **6 possible actions**:

| Action | Description           |
| ------ | --------------------- |
| **0**  | Move South â¬‡ï¸         |
| **1**  | Move North â¬†ï¸         |
| **2**  | Move East â¡ï¸          |
| **3**  | Move West â¬…ï¸          |
| **4**  | Pick up passenger ğŸš•  |
| **5**  | Drop off passenger ğŸ¯ |

---

### ğŸ’¡ Rewards

| Action Type             | Reward  |
| ----------------------- | ------- |
| Successful drop-off     | **+20** |
| Each time step          | **-1**  |
| Invalid pickup/drop-off | **-10** |

A successful episode means picking up and dropping off a passenger correctly and efficiently to maximize cumulative reward. ğŸ…

---

### ğŸ”„ Transition Probabilities

In reinforcement learning, the **transition probabilities** define how likely it is to move from one state to another after taking a specific action.

In Taxi-v3, these are accessible via:

```python
env.unwrapped.P[s][a]
```

Each entry gives a list of tuples `(probability, next_state, reward, terminated)` for all possible outcomes from state `s` with action `a`.

Example:

```python
env.unwrapped.P[1][4]
```

returns all transitions from state **1** after taking **action 4 (pickup)**.

These probabilities are essential for computing expected returns in **value iteration**.

---
